{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# parsing raw spike times from hdf5 (runtime ~7 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T19:17:35.833144Z",
     "start_time": "2021-03-03T19:17:23.740354Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#automate the boring stuff\n",
    "from lib.getterdone import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T19:17:44.907038Z",
     "start_time": "2021-03-03T19:17:44.737833Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T20:50:42.178985Z",
     "start_time": "2021-03-11T20:50:20.387074Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#search_for_filename\n",
    "input_file_name=search_for_file()\n",
    "#manual data has an ugly dtype\n",
    "# input_file_name='/Users/timothytyree/Documents/GitHub/neurophysics/notebooks/Data/SRT_CollabData/SortedData_Manual/catTempPopStorage_full.mat'\n",
    "#early autogen data\n",
    "#Archie\n",
    "# input_file_name='/Users/timothytyree/Documents/GitHub/neurophysics/notebooks/Data/SRT_CollabData/SortedData_AutoGen/Archie_SRT_Set226_subset1_201020_142509_UnitStorage.mat'\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:21:46.569478Z",
     "start_time": "2021-03-03T19:17:47.852498Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/timothytyree/Documents/GitHub/neurophysics/notebooks/Data/Archie/spike_times_trial_451.json\n"
     ]
    }
   ],
   "source": [
    "#extract all spike times for all trials in .mat\n",
    "# field_lst=['chanNum', 'featSpace', 'spikeInds', 'spikeTimes', 'trialFR', 'trialPSTH', 'trialSpks', 'wvfrms']\n",
    "with h5py.File(input_file_name, 'r') as f:\n",
    "    obj=f['SRTunitStorage']['trialSpks']\n",
    "    num_neurons=obj.shape[0]\n",
    "    neuron_id_lst=sorted(range(num_neurons))\n",
    "    for trial_id in sorted(range(53,452)):\n",
    "        spikes_lst=[]\n",
    "        for nid in neuron_id_lst:\n",
    "            ref=obj[(nid)][0]\n",
    "            name = h5py.h5r.get_name(ref, f.id)\n",
    "            trials = np.array(f[(name)]).flatten();#print(name) #not sure if trials covers trials or neurons... its length is ~450.\n",
    "            ref=trials[trial_id]\n",
    "            name = h5py.h5r.get_name(ref, f.id)\n",
    "            spikes=np.array(f[(name)]);#print(f\"{nid}:{name},\")\n",
    "            spikes_lst.append(spikes)\n",
    "\n",
    "        dict_spike_times={}\n",
    "        for nid,spikes in zip(neuron_id_lst,spikes_lst):\n",
    "            dict_spike_times[nid]=list(spikes.flatten().astype(np.float64))\n",
    "\n",
    "        # the json file where the output must be stored \n",
    "#         os.chdir(f\"{nb_dir}/Data/Archie\")\n",
    "        os.chdir(f\"{nb_dir}/Data/Archie\")\n",
    "        out_fn=f\"spike_times_trial_{trial_id}.json\"\n",
    "        out_file = open(out_fn, \"w\") \n",
    "        json.dump(dict_spike_times, out_file, indent = 6) \n",
    "        out_file.close() \n",
    "\n",
    "print(os.path.abspath(out_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- % SRTunitStorage – 1 x n Neurons struct\n",
    "- %     % chanNum – channel ID number Neuron was located on\n",
    "- %     % spikeInds – Index of Spikes from that neuron (in 30kHz recording time)\n",
    "- %     % spikeTimes – Index of Spikes converted to Time (sec), relative to recording start\n",
    "- %     % trialSpks – SpikeTimes for each individual Trial, Time relative to Stimlus onset (sec)\n",
    "- %     % trialFR – 1 x nTrials cell array \n",
    "- %     % \t Cell is 49 bins, ea the instantaneuous firing rate (Hz) relative to stimulus onset\n",
    "- %     % \tI believe they are 100msec bins, -2sec to +3 sec\n",
    "- %     % trialPSTH – Histogram of firing relative to Stimulus onset per Trial\n",
    "- %     % \t100ms bins, raw n spikes (not converted to Hz) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#TODO: import mike's .mat file\n",
    "#Extract the fields for one trial\n",
    "import h5py\n",
    "trial_id=-1\n",
    "field_lst=['chanNum', 'featSpace', 'spikeInds', 'spikeTimes', 'trialFR', 'trialPSTH', 'trialSpks', 'wvfrms']\n",
    "with h5py.File(input_file_name, 'r') as f:\n",
    "#     print(f.keys())\n",
    "#     print(mat:=f['SRTunitStorage'].keys())#.fletcher32)#.items())#__dir__())\n",
    "#     print(obj:=f['SRTunitStorage']['spikeTimes'])\n",
    "\n",
    "    trial=f['SRTunitStorage']['spikeTimes'][(trial_id)][0]\n",
    "    name = h5py.h5r.get_name(trial, f.id)\n",
    "    data = np.array(f[(name)])\n",
    "    ref=f['SRTunitStorage']['chanNum'][(trial_id)][0]\n",
    "    name = h5py.h5r.get_name(ref, f.id)\n",
    "    chnlno = int(np.array(f[(name)]))\n",
    "    \n",
    "    obj=f['SRTunitStorage']['featSpace'][(trial_id)]\n",
    "    print(obj.shape)\n",
    "    ref=obj[0]\n",
    "    name = h5py.h5r.get_name(ref, f.id)\n",
    "    featSpace = np.array(f[(name)])\n",
    "    \n",
    "    obj=f['SRTunitStorage']['spikeInds'][(trial_id)]\n",
    "    print(obj.shape)\n",
    "    ref=obj[0]\n",
    "    name = h5py.h5r.get_name(ref, f.id)\n",
    "    spikeInds = np.array(f[(name)])\n",
    "    \n",
    "    obj=f['SRTunitStorage']['trialSpks'][(trial_id)]\n",
    "    print(obj.shape)\n",
    "    ref=obj[0]\n",
    "    name = h5py.h5r.get_name(ref, f.id)\n",
    "    trialSpks = np.array(f[(name)])\n",
    "    \n",
    "#     chnlno_lst=[]\n",
    "#     for trial_id in range(195):\n",
    "#         ref=f['SRTunitStorage']['chanNum'][(trial_id)][0]\n",
    "#         name = h5py.h5r.get_name(ref, f.id)\n",
    "#         chnlno = int(np.array(f[(name)]))\n",
    "#         chnlno_lst.append(chnlno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# (deprecated) parse raw spike times from csv (runtime <2 mintues)\n",
    "this schema is described in doc/schema.docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T05:44:03.420210Z",
     "start_time": "2021-06-06T05:43:52.380435Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from lib.getterdone import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T18:40:45.876047Z",
     "start_time": "2021-06-06T18:40:45.858471Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpikeTimes.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 226]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the raw data files of interest\n",
    "data_dir=f\"{nb_dir}/Data/Archie\"\n",
    "# data_dir=f\"{nb_dir}/Data/Buck\"\n",
    "\n",
    "# TODO: parse the following... the method needs editing to notice the right files\n",
    "# data_dir=f\"{nb_dir}/Data/Baloo\"\n",
    "# data_dir=f\"{nb_dir}/Data/Hades\"\n",
    "# data_dir=f\"{nb_dir}/Data/Karl\"\n",
    "os.chdir(data_dir)\n",
    "fn_lst=sorted([fn for fn in os.listdir() if fn[-4:]=='.csv'])\n",
    "trgt1='trialTimes.csv'\n",
    "trgt2='trialData.csv'\n",
    "trgt3='ChannelNums.csv'\n",
    "trgt4='SpikeTimes.csv'\n",
    "input_fn_lst=[]\n",
    "setnum_lst=[]\n",
    "trgt=trgt4\n",
    "for fn in fn_lst:\n",
    "    if fn.find(trgt)!=-1:\n",
    "        input_fn_lst.append(fn)\n",
    "        setnum=eval(fn.split('_')[2].lower().split('set')[-1])\n",
    "        setnum_lst.append(setnum)\n",
    "print(trgt)\n",
    "setnum_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T18:42:15.828164Z",
     "start_time": "2021-06-06T18:41:11.219800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#import all files of raw data and convert it into a dictionary of spike times (may take ~1 minute)\n",
    "dict_spike_times_lst=[]\n",
    "for input_fn in input_fn_lst:\n",
    "    dict_spike_times=dict()\n",
    "    with open(input_fn) as f:\n",
    "        nid=0\n",
    "        for line in f:\n",
    "            spikes=[]\n",
    "            for val in line.split(','):\n",
    "                try:\n",
    "                    val=eval(val)\n",
    "                    spikes.append(val)\n",
    "                except SyntaxError as e:\n",
    "                    pass\n",
    "            dict_spike_times[nid]=spikes\n",
    "            nid+=1\n",
    "    dict_spike_times_lst.append(dict_spike_times)\n",
    "raw=dict(zip(trialnum_lst,dict_spike_times_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#TODO: parse trialTimes for each set\n",
    "#TODO(later): make parser for _trialData.csv into a pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T05:54:16.041226Z",
     "start_time": "2021-06-06T05:54:08.553670Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# out_fn=f\"archie_raw.json\"\n",
    "out_fn=f\"buck_raw.json\"\n",
    "#save as json\n",
    "save_folder=f\"{nb_dir}/Data\"\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "os.chdir(save_folder)\n",
    "out_file = open(out_fn, \"w\")\n",
    "json.dump(raw, out_file, indent = 6)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#TODO: restart JVM\n",
    "#TODO: import trialtimes\n",
    "#TODO: get parsing working for \n",
    "#TODO: format trialtimes into labeled vector for (red/yellow/blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T05:54:28.969609Z",
     "start_time": "2021-06-06T05:54:28.760918Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:26:09.471086Z",
     "start_time": "2021-06-06T00:26:09.468982Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# #parse the file names\n",
    "# data_dir=f\"{nb_dir}/Data/gitignored\"\n",
    "# os.chdir(data_dir)\n",
    "# fn_lst=sorted([fn for fn in os.listdir() if fn[-4:]=='.csv'])\n",
    "# # print(len(fn_lst))\n",
    "\n",
    "# #TODO: import a given csv\n",
    "# fn=fn_lst[0]\n",
    "# MonkName,ProjectName,StimulusSet,Subset,YYMMDD,HHMMSS,modname=fn.split('_')\n",
    "# print(modname)\n",
    "\n",
    "\n",
    "# setnumber_lst=[]\n",
    "# for fn in fn_lst:\n",
    "#     setnumber=eval(fn.split('_')[2].lower().split('set')[-1])\n",
    "#     setnumber_lst.append(setnumber)\n",
    "# setnumber_set=set(sorted(set(setnumber_lst)))\n",
    "# #TODO: make a function mapping setnumber set to a desired .csv file name\n",
    "# # setnumber=setnumber_set[0]\n",
    "# fn_foo=lambda setnumber,modename:f\"{MonkName}_{ProjectName}_Set{setnumber}_{YYMMDD}_{HHMMSS}_{modname}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:32:06.876467Z",
     "start_time": "2021-06-06T00:32:06.870316Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# num_neurons=obj.shape[0]\n",
    "\n",
    "\n",
    "# neuron_id_lst=sorted(range(num_neurons))\n",
    "# for trial_id in sorted(range(53,452)):\n",
    "#     spikes_lst=[]\n",
    "#     for nid in neuron_id_lst:\n",
    "#         ref=obj[(nid)][0]\n",
    "#         name = h5py.h5r.get_name(ref, f.id)\n",
    "#         trials = np.array(f[(name)]).flatten();#print(name) #not sure if trials covers trials or neurons... its length is ~450.\n",
    "#         ref=trials[trial_id]\n",
    "#         name = h5py.h5r.get_name(ref, f.id)\n",
    "#         spikes=np.array(f[(name)]);#print(f\"{nid}:{name},\")\n",
    "#         spikes_lst.append(spikes)\n",
    "\n",
    "#     dict_spike_times={}\n",
    "#     for nid,spikes in zip(neuron_id_lst,spikes_lst):\n",
    "#         dict_spike_times[nid]=list(spikes.flatten().astype(np.float64))\n",
    "\n",
    "#     # the json file where the output must be stored \n",
    "# #         os.chdir(f\"{nb_dir}/Data/Archie\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
